{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_intuition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praths007/machine_learning_intuition/blob/master/nn_intuition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dEpepqpS0ajx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Understanding neural networks using gradient descent**"
      ]
    },
    {
      "metadata": {
        "id": "XZjY50Wn9ybg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fc88b8ed-5c9a-4343-d654-611df8a2720a"
      },
      "cell_type": "code",
      "source": [
        "rm(list = ls())\n",
        "### caret only used for stratified random sampling\n",
        "library(caret)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading required package: lattice\n",
            "Loading required package: ggplot2\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "TWmuN2lP97ID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "93ee8836-5052-495e-857e-873a92c8f9de"
      },
      "cell_type": "code",
      "source": [
        "### loading dataset\n",
        "############################################\n",
        "iris_data = iris[which(iris$Species %in% c(\"setosa\", \"versicolor\")),]\n",
        "## not used 1 and 0 because array indexes in R start from 1.\n",
        "## this will ease the computation while creating seq...\n",
        "## also this can be used to do multiclass classification\n",
        "## where labels could be 1,2,3 etc...\n",
        "\n",
        "iris_data$Species = ifelse(iris_data$Species == \"setosa\", 1, 2)\n",
        "\n",
        "\n",
        "### adding intercept column to the data\n",
        "iris_data$intercept = 1\n",
        "\n",
        "iris_data = cbind(intercept = iris_data$intercept, \n",
        "                  iris_data[,c(\"Petal.Length\", \"Petal.Width\", \"Species\")])\n",
        "\n",
        "index = unlist(createDataPartition(iris_data$Species, p =  0.7))\n",
        "\n",
        "train_x = data.matrix(iris_data[index,c(\"intercept\", \"Petal.Length\", \"Petal.Width\")])\n",
        "\n",
        "train_y = data.matrix(iris_data[index,c(\"Species\")])\n",
        "\n",
        "\n",
        "test_x = data.matrix(iris_data[-index, c(\"intercept\", \"Petal.Length\", \"Petal.Width\")])\n",
        "\n",
        "test_y = data.matrix(iris_data[-index, c(\"Species\")])\n",
        "\n",
        "nrow(train_x)\n",
        "nrow(test_x)\n",
        "############################################"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] 70"
            ],
            "text/latex": "70",
            "text/markdown": "70",
            "text/html": [
              "70"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] 30"
            ],
            "text/latex": "30",
            "text/markdown": "30",
            "text/html": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tkEtoH2f-AWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### defining required functions\n",
        "############################################\n",
        "\n",
        "squared_error_cost = function(y, yhat){\n",
        "  return((1 / nrow(y)) * sum((y - yhat)^2))}\n",
        "\n",
        "## evaluation metric/ cost function logloss is used for classification\n",
        "## https://datawookie.netlify.com/blog/2015/12/making-sense-of-logarithmic-loss/\n",
        "logloss_cost = function(y, yhat){\n",
        "  return(-(1 / nrow(y)) * sum((y*log(yhat) + (1 - y)*log(1 - yhat))))}\n",
        "\n",
        "## activation function - used to limit values between 0 and 1\n",
        "sigmoid = function(x){\n",
        "  return(1 / (1 + 2.71^-x))}\n",
        "\n",
        "## used for backpropogation\n",
        "sigmoidGradient = function(x){\n",
        "  return(sigmoid(x) * (1 - sigmoid(x)))}\n",
        "############################################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7H35ehcf-Kot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1317
        },
        "outputId": "93127cbe-cb2f-40c3-eb25-6259797a9aae"
      },
      "cell_type": "code",
      "source": [
        "#### neural networks intuition\n",
        "################################################\n",
        "\n",
        "num_labels = 2 # 1 and 2 in our case\n",
        "\n",
        "m = dim(train_x)[1]\n",
        "n = dim(train_x)[2]\n",
        "\n",
        "## number of nodes in 1st layer/input layer must\n",
        "## equal number of columns/ features of input data\n",
        "layer1_nodes = ncol(train_x)\n",
        "\n",
        "## this layer can have as many nodes as required\n",
        "## rule of thumb is usually 1.5 times or equal number\n",
        "## this is the hidden layer\n",
        "layer2_nodes = trunc(ncol(train_x) * 1.5)\n",
        "\n",
        "# final output layer nodes = number of labels\n",
        "output_layer_nodes = num_labels\n",
        "\n",
        "\n",
        "Theta1 = array(runif(n = gaussian(), min = 0, max = 1), c(layer2_nodes, layer1_nodes))\n",
        "\n",
        "Theta2 = array(runif(n = gaussian(), min = 0, max = 1), c(output_layer_nodes, layer2_nodes+1))\n",
        "\n",
        "## input layer\n",
        "a1 = train_x\n",
        "\n",
        "z2 = a1 %*% t(Theta1)\n",
        "\n",
        "a2 = sigmoid(z2)\n",
        "\n",
        "# second layer (hidden layer)\n",
        "a2 = cbind(1, a2)\n",
        "\n",
        "z3 = a2 %*% t(Theta2)\n",
        "\n",
        "# third layer (output layer)\n",
        "a3 = sigmoid(z3)\n",
        "\n",
        "## neural network cost:\n",
        "# it is a triple summation of logistic cost (because we are using sigmoid as activation),\n",
        "# where the innermost summation is sum of cost for each node within a layer\n",
        "# think of this as a collection of logistic regression models where each node represents\n",
        "# one logit model (having sigmoid as its activation, similar to what was done in logit intuition)\n",
        "\n",
        "individual_train = array(0L, m)\n",
        "individual_theta_k = array(0L, output_layer_nodes)\n",
        "for(i in seq(1, m)){\n",
        "  # for calculating innermost summation of cost (which is basically cost/error\n",
        "  # for each node of the output layer wrt actual value)\n",
        "  for(k in seq(1, num_labels)){\n",
        "    individual_theta_k[k] = as.numeric(train_y[i,] == k)  * log(a3[i, k]) + \n",
        "                          (1 - as.numeric(train_y[i,] == k)) * log(1 - a3[i, k])\n",
        "  }\n",
        "  # summation of errors at each label K for each input tuple/ row\n",
        "  individual_train[i] = sum(individual_theta_k)\n",
        "}\n",
        "# summation of errors for all tuples\n",
        "cost = -(1/m) * sum(individual_train)\n",
        "cost\n",
        "\n",
        "\n",
        "## backpropogation (adjusting weights of previous layers wrt error in output layer)\n",
        "## delta3 = error in output layer\n",
        "## again done for each node in the layer\n",
        "delta3 = array(0L, c(dim(a3)))\n",
        "for(i in seq(1, m)){\n",
        "  for(k in seq(1, num_labels)){\n",
        "    delta3[i, k] = a3[i, k] - as.numeric(train_y[i,] == k)\n",
        "  }\n",
        "}\n",
        "\n",
        "# this is the error\n",
        "delta3\n",
        "\n",
        "# we will be taking a negative gradient of this error/ cost to calculate the\n",
        "# direction of steepest ascent and then adjust weights for each node of each layer\n",
        "# accordingly\n",
        "\n",
        "delta2 = (delta3 %*% Theta2[,2:ncol(Theta2)]) * sigmoidGradient(z2)\n",
        "\n",
        "Theta1_grad = (1 / m) * t(delta2) %*% a1\n",
        "Theta2_grad = (1 / m) * t(delta3) %*% a2\n",
        "  \n",
        "alpha = 0.001\n",
        "Theta1 = Theta1 - alpha * Theta1_grad\n",
        "Theta2 = Theta2 - alpha * Theta2_grad"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] 2.767547"
            ],
            "text/latex": "2.76754690337835",
            "text/markdown": "2.76754690337835",
            "text/html": [
              "2.76754690337835"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      [,1]        [,2]       \n",
              " [1,] -0.07611479 0.9034793  \n",
              " [2,] -0.07611479 0.9034793  \n",
              " [3,] -0.07490820 0.9047159  \n",
              " [4,] -0.07611479 0.9034793  \n",
              " [5,] -0.06854687 0.9119436  \n",
              " [6,] -0.07376584 0.9062052  \n",
              " [7,] -0.07611479 0.9034793  \n",
              " [8,] -0.07733222 0.9019046  \n",
              " [9,] -0.07490820 0.9047159  \n",
              "[10,] -0.07374733 0.9059134  \n",
              "[11,] -0.07052133 0.9098663  \n",
              "[12,] -0.07265997 0.9076473  \n",
              "[13,] -0.07051911 0.9095807  \n",
              "[14,] -0.07264063 0.9073673  \n",
              "[15,] -0.06670605 0.9141688  \n",
              "[16,] -0.07374733 0.9059134  \n",
              "[17,] -0.06951423 0.9109222  \n",
              "[18,] -0.07490820 0.9047159  \n",
              "[19,] -0.07611479 0.9034793  \n",
              "[20,] -0.07374733 0.9059134  \n",
              "[21,] -0.07374733 0.9059134  \n",
              "[22,] -0.07733222 0.9019046  \n",
              "[23,] -0.07611479 0.9034793  \n",
              "[24,] -0.07867001 0.9008850  \n",
              "[25,] -0.07736832 0.9022026  \n",
              "[26,] -0.07736832 0.9022026  \n",
              "[27,] -0.07490820 0.9047159  \n",
              "[28,] -0.07493572 0.9050048  \n",
              "[29,] -0.07493572 0.9050048  \n",
              "[30,] -0.07736832 0.9022026  \n",
              "[31,] ⋮           ⋮          \n",
              "[32,] 0.9484696   -0.06733123\n",
              "[33,] 0.9526079   -0.06123824\n",
              "[34,] 0.9505466   -0.06426576\n",
              "[35,] 0.9525084   -0.06144564\n",
              "[36,] 0.9530484   -0.06059647\n",
              "[37,] 0.9500094   -0.06527941\n",
              "[38,] 0.9530484   -0.06059647\n",
              "[39,] 0.9501178   -0.06503449\n",
              "[40,] 0.9544118   -0.05840459\n",
              "[41,] 0.9513740   -0.06313010\n",
              "[42,] 0.9535674   -0.05982152\n",
              "[43,] 0.9521401   -0.06211232\n",
              "[44,] 0.9519176   -0.06236331\n",
              "[45,] 0.9525084   -0.06144564\n",
              "[46,] 0.9530834   -0.06060017\n",
              "[47,] 0.9543197   -0.05858476\n",
              "[48,] 0.9530484   -0.06059647\n",
              "[49,] 0.9490236   -0.06660442\n",
              "[50,] 0.9506648   -0.06419533\n",
              "[51,] 0.9541196   -0.05893320\n",
              "[52,] 0.9530484   -0.06059647\n",
              "[53,] 0.9520858   -0.06212238\n",
              "[54,] 0.9515620   -0.06286690\n",
              "[55,] 0.9513740   -0.06313010\n",
              "[56,] 0.9528061   -0.06101087\n",
              "[57,] 0.9508742   -0.06390709\n",
              "[58,] 0.9478696   -0.06810542\n",
              "[59,] 0.9512700   -0.06335595\n",
              "[60,] 0.9476432   -0.06826698\n",
              "[61,] 0.9515620   -0.06286690"
            ],
            "text/latex": "\\begin{tabular}{ll}\n\t -0.07611479 & 0.9034793  \\\\\n\t -0.07611479 & 0.9034793  \\\\\n\t -0.07490820 & 0.9047159  \\\\\n\t -0.07611479 & 0.9034793  \\\\\n\t -0.06854687 & 0.9119436  \\\\\n\t -0.07376584 & 0.9062052  \\\\\n\t -0.07611479 & 0.9034793  \\\\\n\t -0.07733222 & 0.9019046  \\\\\n\t -0.07490820 & 0.9047159  \\\\\n\t -0.07374733 & 0.9059134  \\\\\n\t -0.07052133 & 0.9098663  \\\\\n\t -0.07265997 & 0.9076473  \\\\\n\t -0.07051911 & 0.9095807  \\\\\n\t -0.07264063 & 0.9073673  \\\\\n\t -0.06670605 & 0.9141688  \\\\\n\t -0.07374733 & 0.9059134  \\\\\n\t -0.06951423 & 0.9109222  \\\\\n\t -0.07490820 & 0.9047159  \\\\\n\t -0.07611479 & 0.9034793  \\\\\n\t -0.07374733 & 0.9059134  \\\\\n\t -0.07374733 & 0.9059134  \\\\\n\t -0.07733222 & 0.9019046  \\\\\n\t -0.07611479 & 0.9034793  \\\\\n\t -0.07867001 & 0.9008850  \\\\\n\t -0.07736832 & 0.9022026  \\\\\n\t -0.07736832 & 0.9022026  \\\\\n\t -0.07490820 & 0.9047159  \\\\\n\t -0.07493572 & 0.9050048  \\\\\n\t -0.07493572 & 0.9050048  \\\\\n\t -0.07736832 & 0.9022026  \\\\\n\t ⋮ & ⋮\\\\\n\t 0.9484696   & -0.06733123\\\\\n\t 0.9526079   & -0.06123824\\\\\n\t 0.9505466   & -0.06426576\\\\\n\t 0.9525084   & -0.06144564\\\\\n\t 0.9530484   & -0.06059647\\\\\n\t 0.9500094   & -0.06527941\\\\\n\t 0.9530484   & -0.06059647\\\\\n\t 0.9501178   & -0.06503449\\\\\n\t 0.9544118   & -0.05840459\\\\\n\t 0.9513740   & -0.06313010\\\\\n\t 0.9535674   & -0.05982152\\\\\n\t 0.9521401   & -0.06211232\\\\\n\t 0.9519176   & -0.06236331\\\\\n\t 0.9525084   & -0.06144564\\\\\n\t 0.9530834   & -0.06060017\\\\\n\t 0.9543197   & -0.05858476\\\\\n\t 0.9530484   & -0.06059647\\\\\n\t 0.9490236   & -0.06660442\\\\\n\t 0.9506648   & -0.06419533\\\\\n\t 0.9541196   & -0.05893320\\\\\n\t 0.9530484   & -0.06059647\\\\\n\t 0.9520858   & -0.06212238\\\\\n\t 0.9515620   & -0.06286690\\\\\n\t 0.9513740   & -0.06313010\\\\\n\t 0.9528061   & -0.06101087\\\\\n\t 0.9508742   & -0.06390709\\\\\n\t 0.9478696   & -0.06810542\\\\\n\t 0.9512700   & -0.06335595\\\\\n\t 0.9476432   & -0.06826698\\\\\n\t 0.9515620   & -0.06286690\\\\\n\\end{tabular}\n",
            "text/markdown": "\n| -0.07611479 | 0.9034793   |\n| -0.07611479 | 0.9034793   |\n| -0.07490820 | 0.9047159   |\n| -0.07611479 | 0.9034793   |\n| -0.06854687 | 0.9119436   |\n| -0.07376584 | 0.9062052   |\n| -0.07611479 | 0.9034793   |\n| -0.07733222 | 0.9019046   |\n| -0.07490820 | 0.9047159   |\n| -0.07374733 | 0.9059134   |\n| -0.07052133 | 0.9098663   |\n| -0.07265997 | 0.9076473   |\n| -0.07051911 | 0.9095807   |\n| -0.07264063 | 0.9073673   |\n| -0.06670605 | 0.9141688   |\n| -0.07374733 | 0.9059134   |\n| -0.06951423 | 0.9109222   |\n| -0.07490820 | 0.9047159   |\n| -0.07611479 | 0.9034793   |\n| -0.07374733 | 0.9059134   |\n| -0.07374733 | 0.9059134   |\n| -0.07733222 | 0.9019046   |\n| -0.07611479 | 0.9034793   |\n| -0.07867001 | 0.9008850   |\n| -0.07736832 | 0.9022026   |\n| -0.07736832 | 0.9022026   |\n| -0.07490820 | 0.9047159   |\n| -0.07493572 | 0.9050048   |\n| -0.07493572 | 0.9050048   |\n| -0.07736832 | 0.9022026   |\n| ⋮ | ⋮ |\n| 0.9484696   | -0.06733123 |\n| 0.9526079   | -0.06123824 |\n| 0.9505466   | -0.06426576 |\n| 0.9525084   | -0.06144564 |\n| 0.9530484   | -0.06059647 |\n| 0.9500094   | -0.06527941 |\n| 0.9530484   | -0.06059647 |\n| 0.9501178   | -0.06503449 |\n| 0.9544118   | -0.05840459 |\n| 0.9513740   | -0.06313010 |\n| 0.9535674   | -0.05982152 |\n| 0.9521401   | -0.06211232 |\n| 0.9519176   | -0.06236331 |\n| 0.9525084   | -0.06144564 |\n| 0.9530834   | -0.06060017 |\n| 0.9543197   | -0.05858476 |\n| 0.9530484   | -0.06059647 |\n| 0.9490236   | -0.06660442 |\n| 0.9506648   | -0.06419533 |\n| 0.9541196   | -0.05893320 |\n| 0.9530484   | -0.06059647 |\n| 0.9520858   | -0.06212238 |\n| 0.9515620   | -0.06286690 |\n| 0.9513740   | -0.06313010 |\n| 0.9528061   | -0.06101087 |\n| 0.9508742   | -0.06390709 |\n| 0.9478696   | -0.06810542 |\n| 0.9512700   | -0.06335595 |\n| 0.9476432   | -0.06826698 |\n| 0.9515620   | -0.06286690 |\n\n",
            "text/html": [
              "<table>\n",
              "<tbody>\n",
              "\t<tr><td>-0.07611479</td><td>0.9034793  </td></tr>\n",
              "\t<tr><td>-0.07611479</td><td>0.9034793  </td></tr>\n",
              "\t<tr><td>-0.07490820</td><td>0.9047159  </td></tr>\n",
              "\t<tr><td>-0.07611479</td><td>0.9034793  </td></tr>\n",
              "\t<tr><td>-0.06854687</td><td>0.9119436  </td></tr>\n",
              "\t<tr><td>-0.07376584</td><td>0.9062052  </td></tr>\n",
              "\t<tr><td>-0.07611479</td><td>0.9034793  </td></tr>\n",
              "\t<tr><td>-0.07733222</td><td>0.9019046  </td></tr>\n",
              "\t<tr><td>-0.07490820</td><td>0.9047159  </td></tr>\n",
              "\t<tr><td>-0.07374733</td><td>0.9059134  </td></tr>\n",
              "\t<tr><td>-0.07052133</td><td>0.9098663  </td></tr>\n",
              "\t<tr><td>-0.07265997</td><td>0.9076473  </td></tr>\n",
              "\t<tr><td>-0.07051911</td><td>0.9095807  </td></tr>\n",
              "\t<tr><td>-0.07264063</td><td>0.9073673  </td></tr>\n",
              "\t<tr><td>-0.06670605</td><td>0.9141688  </td></tr>\n",
              "\t<tr><td>-0.07374733</td><td>0.9059134  </td></tr>\n",
              "\t<tr><td>-0.06951423</td><td>0.9109222  </td></tr>\n",
              "\t<tr><td>-0.07490820</td><td>0.9047159  </td></tr>\n",
              "\t<tr><td>-0.07611479</td><td>0.9034793  </td></tr>\n",
              "\t<tr><td>-0.07374733</td><td>0.9059134  </td></tr>\n",
              "\t<tr><td>-0.07374733</td><td>0.9059134  </td></tr>\n",
              "\t<tr><td>-0.07733222</td><td>0.9019046  </td></tr>\n",
              "\t<tr><td>-0.07611479</td><td>0.9034793  </td></tr>\n",
              "\t<tr><td>-0.07867001</td><td>0.9008850  </td></tr>\n",
              "\t<tr><td>-0.07736832</td><td>0.9022026  </td></tr>\n",
              "\t<tr><td>-0.07736832</td><td>0.9022026  </td></tr>\n",
              "\t<tr><td>-0.07490820</td><td>0.9047159  </td></tr>\n",
              "\t<tr><td>-0.07493572</td><td>0.9050048  </td></tr>\n",
              "\t<tr><td>-0.07493572</td><td>0.9050048  </td></tr>\n",
              "\t<tr><td>-0.07736832</td><td>0.9022026  </td></tr>\n",
              "\t<tr><td>⋮</td><td>⋮</td></tr>\n",
              "\t<tr><td>0.9484696  </td><td>-0.06733123</td></tr>\n",
              "\t<tr><td>0.9526079  </td><td>-0.06123824</td></tr>\n",
              "\t<tr><td>0.9505466  </td><td>-0.06426576</td></tr>\n",
              "\t<tr><td>0.9525084  </td><td>-0.06144564</td></tr>\n",
              "\t<tr><td>0.9530484  </td><td>-0.06059647</td></tr>\n",
              "\t<tr><td>0.9500094  </td><td>-0.06527941</td></tr>\n",
              "\t<tr><td>0.9530484  </td><td>-0.06059647</td></tr>\n",
              "\t<tr><td>0.9501178  </td><td>-0.06503449</td></tr>\n",
              "\t<tr><td>0.9544118  </td><td>-0.05840459</td></tr>\n",
              "\t<tr><td>0.9513740  </td><td>-0.06313010</td></tr>\n",
              "\t<tr><td>0.9535674  </td><td>-0.05982152</td></tr>\n",
              "\t<tr><td>0.9521401  </td><td>-0.06211232</td></tr>\n",
              "\t<tr><td>0.9519176  </td><td>-0.06236331</td></tr>\n",
              "\t<tr><td>0.9525084  </td><td>-0.06144564</td></tr>\n",
              "\t<tr><td>0.9530834  </td><td>-0.06060017</td></tr>\n",
              "\t<tr><td>0.9543197  </td><td>-0.05858476</td></tr>\n",
              "\t<tr><td>0.9530484  </td><td>-0.06059647</td></tr>\n",
              "\t<tr><td>0.9490236  </td><td>-0.06660442</td></tr>\n",
              "\t<tr><td>0.9506648  </td><td>-0.06419533</td></tr>\n",
              "\t<tr><td>0.9541196  </td><td>-0.05893320</td></tr>\n",
              "\t<tr><td>0.9530484  </td><td>-0.06059647</td></tr>\n",
              "\t<tr><td>0.9520858  </td><td>-0.06212238</td></tr>\n",
              "\t<tr><td>0.9515620  </td><td>-0.06286690</td></tr>\n",
              "\t<tr><td>0.9513740  </td><td>-0.06313010</td></tr>\n",
              "\t<tr><td>0.9528061  </td><td>-0.06101087</td></tr>\n",
              "\t<tr><td>0.9508742  </td><td>-0.06390709</td></tr>\n",
              "\t<tr><td>0.9478696  </td><td>-0.06810542</td></tr>\n",
              "\t<tr><td>0.9512700  </td><td>-0.06335595</td></tr>\n",
              "\t<tr><td>0.9476432  </td><td>-0.06826698</td></tr>\n",
              "\t<tr><td>0.9515620  </td><td>-0.06286690</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "onUAWBTw0ajy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "e714c4e6-2ef0-4292-dc70-ebf22d0cf653"
      },
      "cell_type": "code",
      "source": [
        "## gradient descent intuition\n",
        "\n",
        "cost = c()\n",
        "num_labels = 2 # 0 and 1 in our case\n",
        "\n",
        "m = dim(train_x)[1]\n",
        "n = dim(train_x)[2]\n",
        "\n",
        "## number of nodes in 1st layer/input layer must\n",
        "## equal number of columns/ features of input data\n",
        "layer1_nodes = ncol(train_x)\n",
        "\n",
        "## this layer can have as many nodes as required\n",
        "## rule of thumb is usually 1.5 times or equal number\n",
        "layer2_nodes = trunc(ncol(train_x) * 1.5)\n",
        "\n",
        "# final output layer nodes = number of labels\n",
        "output_layer_nodes = 2\n",
        "\n",
        "gradient_descent_for_nn = function(alpha, iterations, train_x, train_y)\n",
        "{\n",
        "  ## initialize theta (weights)\n",
        "  \n",
        "  Theta1 = array(runif(n = gaussian(), min = 0, max = 1), c(layer2_nodes, layer1_nodes))\n",
        "  \n",
        "  Theta2 = array(runif(n = gaussian(), min = 0, max = 1), c(output_layer_nodes, layer2_nodes+1))\n",
        "  \n",
        "  \n",
        "  for(i in seq(1, iterations)){\n",
        "    \n",
        "    a1 = train_x\n",
        "    \n",
        "    z2 = a1 %*% t(Theta1)\n",
        "    \n",
        "    a2 = sigmoid(z2)\n",
        "    \n",
        "    a2 = cbind(1, a2)\n",
        "    \n",
        "    \n",
        "    z3 = a2 %*% t(Theta2)\n",
        "    \n",
        "    a3 = sigmoid(z3)\n",
        "    \n",
        "    \n",
        "    individual_train = array(0L, m)\n",
        "    individual_theta_k = array(0L, output_layer_nodes)\n",
        "    for(i in seq(1, m)){\n",
        "      for(k in seq(1, num_labels)){\n",
        "        individual_theta_k[k] = as.numeric(train_y[i,] == k)  * log(a3[i, k]) + \n",
        "          (1 - as.numeric(train_y[i,] == k)) * log(1 - a3[i, k])\n",
        "      }\n",
        "      individual_train[i] = sum(individual_theta_k)\n",
        "    }\n",
        "    cost <<- c(cost, -(1/m) * sum(individual_train))\n",
        "    # cost = c(cost, cost)\n",
        "    # print(cost)\n",
        "    \n",
        "    \n",
        "    \n",
        "    delta3 = array(0L, c(dim(a3)))\n",
        "    for(i in seq(1, m)){\n",
        "      for(k in seq(1, num_labels)){\n",
        "        delta3[i, k] = a3[i, k] - as.numeric(train_y[i,] == k)\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    # delta3\n",
        "    \n",
        "    \n",
        "    # delta2 = Theta2*delta3 * (derivative of activation function, which is sigmoid)\n",
        "    # delta2 = Theta2*delta3 * a3 * (1 - a3)\n",
        "    \n",
        "    delta2 = (delta3 %*% Theta2[,2:ncol(Theta2)]) * sigmoidGradient(z2)\n",
        "    \n",
        "    Theta1_grad = (1 / m) * t(delta2) %*% a1\n",
        "    Theta2_grad = (1 / m) * t(delta3) %*% a2\n",
        "    \n",
        "    # alpha = 0.001\n",
        "    Theta1 = Theta1 - alpha * Theta1_grad\n",
        "    Theta2 = Theta2 - alpha * Theta2_grad\n",
        "    \n",
        "  }\n",
        "  return(list(Theta1, Theta2))\n",
        "  \n",
        "}\n",
        "\n",
        "\n",
        "alpha = 0.01\n",
        "## keep changing epochs, more epochs = more steps towards minimum cost\n",
        "epochs = 3000\n",
        "\n",
        "thetas = gradient_descent_for_nn(alpha, epochs, train_x, train_y)\n",
        "\n",
        "plot(cost)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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\nRU5ErkJggg==",
            "text/plain": [
              "plot without title"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yNilwJhf0aj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "a95bfbcf-ea77-4f54-c5dd-775358a471d1"
      },
      "cell_type": "code",
      "source": [
        "## predicting on test set\n",
        "h1 = sigmoid(test_x %*% t(thetas[[1]]))\n",
        "h2 = sigmoid(cbind(1, h1) %*% t(thetas[[2]]))\n",
        "\n",
        "\n",
        "preds = apply(h2, 1, function(x) which(x == max(x)))\n",
        "\n",
        "confusionMatrix(as.factor(preds), as.factor(test_y))\n",
        "## 100% accuracy\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Confusion Matrix and Statistics\n",
              "\n",
              "          Reference\n",
              "Prediction  1  2\n",
              "         1 15  0\n",
              "         2  0 15\n",
              "                                     \n",
              "               Accuracy : 1          \n",
              "                 95% CI : (0.8843, 1)\n",
              "    No Information Rate : 0.5        \n",
              "    P-Value [Acc > NIR] : 9.313e-10  \n",
              "                                     \n",
              "                  Kappa : 1          \n",
              " Mcnemar's Test P-Value : NA         \n",
              "                                     \n",
              "            Sensitivity : 1.0        \n",
              "            Specificity : 1.0        \n",
              "         Pos Pred Value : 1.0        \n",
              "         Neg Pred Value : 1.0        \n",
              "             Prevalence : 0.5        \n",
              "         Detection Rate : 0.5        \n",
              "   Detection Prevalence : 0.5        \n",
              "      Balanced Accuracy : 1.0        \n",
              "                                     \n",
              "       'Positive' Class : 1          \n",
              "                                     "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}